{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "61c06a31-1214-4e4e-8523-085b00abf724"
    }
   },
   "source": [
    "# Data Acquaision (Do not run it Now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1e19a91a-d58a-4674-bac4-d731aa55e096"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "\n",
    "playerID = \"977\"\n",
    "\n",
    "for season in range(1996,2016):\n",
    "    # The stats.nba.com API wants season as \"1996-97\"\n",
    "    seasonString = str(season) + '-' + str(season+1)[2:]\n",
    "    # The stats.nba.com endpoint we are using is http://stats.nba.com/stats/shotchartdetail\n",
    "    # More info on endpoints: https://github.com/seemethere/nba_py/wiki/stats.nba.com-Endpoint-Documentation\n",
    "    shot_chart_url = 'http://stats.nba.com/stats/shotchartdetail?Period=0&VsConference=&LeagueID=00&LastNGames=0&TeamID=0&PlayerPosition=&Position=&Location=&Outcome=&ContextMeasure=FGA&DateFrom=&StartPeriod=&DateTo=&OpponentTeamID=0&ContextFilter=&RangeType=&Season=' + seasonString + '&AheadBehind=&PlayerID=977&EndRange=&VsDivision=&PointDiff=&RookieYear=&GameSegment=&Month=0&ClutchTime=&StartRange=&EndPeriod=&SeasonType=Regular+Season&SeasonSegment=&GameID='\n",
    "    shot_chart_url = 'http://stats.nba.com/stats/shotchartdetail?Period=0&VsConference=&LeagueID=00&LastNGames=0&TeamID=0&PlayerPosition=&Position=&Location=&Outcome=&ContextMeasure=FGA&DateFrom=&StartPeriod=&DateTo=&OpponentTeamID=0&ContextFilter=&RangeType=&Season=' + seasonString + '&AheadBehind=&PlayerID=977&EndRange=&VsDivision=&PointDiff=&RookieYear=&GameSegment=&Month=0&ClutchTime=&StartRange=&EndPeriod=&SeasonType=Playoffs&SeasonSegment=&GameID='\n",
    "    #print(shot_chart_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d7e2809a-7cd9-4811-b0e1-887644954c78"
    }
   },
   "source": [
    "# Data Combination (Do not run it Now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "4f465f09-7386-4f7e-8f02-ed379bf2bf0c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30697\n"
     ]
    }
   ],
   "source": [
    "seasons = []\n",
    "for season in range(1996,2016):\n",
    "    seasonString = './regular/'+str(season) + '.json'\n",
    "    json_data=open(seasonString).read()\n",
    "    data = json.loads(json_data)\n",
    "    # Split response into headers and content\n",
    "    headers = data['resultSets'][0]['headers']\n",
    "    shots = data['resultSets'][0]['rowSet']\n",
    "    # Create pandas dataframe to hold the data\n",
    "    shot_df = pd.DataFrame(shots, columns=headers)\n",
    "    # add extra column for season\n",
    "    shot_df['SEASON'] = str(season) + '-' + str(season+1)[2:]\n",
    "    # add extra column for playoff flag\n",
    "    shot_df['playoffs'] = 0\n",
    "    seasons.append(shot_df)\n",
    "    \n",
    "for season in range(1996,2016):\n",
    "    seasonString = './playoffs/'+str(season) + '.json'\n",
    "    json_data=open(seasonString).read()\n",
    "    data = json.loads(json_data)\n",
    "    # Split response into headers and content\n",
    "    headers = data['resultSets'][0]['headers']\n",
    "    shots = data['resultSets'][0]['rowSet']\n",
    "    # Create pandas dataframe to hold the data\n",
    "    shot_df = pd.DataFrame(shots, columns=headers)\n",
    "    # add extra column for season\n",
    "    shot_df['SEASON'] = str(season) + '-' + str(season+1)[2:]\n",
    "    # add extra column for playoff flag\n",
    "    shot_df['playoffs'] = 1\n",
    "    seasons.append(shot_df)\n",
    "    \n",
    "kobe = pd.concat(seasons)\n",
    "print(len(kobe['GRID_TYPE']))    \n",
    "kobe.to_csv(\"kobe.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribution 1 Sorting and Partition Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "6b461dec-5148-40d6-bd1a-c32d57a9f056"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import copy\n",
    "\n",
    "def sort_and_partition(data, name, n):\n",
    "    stat = {}\n",
    "    for i, (d, x) in enumerate(zip(data[name], data['shot_made_flag'])):\n",
    "        if not (d in stat) :\n",
    "            stat[d] = [0, 0]\n",
    "        if x == 1:\n",
    "            stat[d][0] += 1\n",
    "            stat[d][1] += 1\n",
    "        else:\n",
    "            stat[d][1] += 1\n",
    "    for key, value in stat.items():\n",
    "        stat[key] = value[0] / value[1]\n",
    "    stat = sorted(stat.items(), key=operator.itemgetter(1))\n",
    "    index_map = {}\n",
    "    for i, (key, value) in enumerate(stat):\n",
    "        index_map[key] = math.floor(i / n)\n",
    "    length = math.ceil(len(stat) / n)\n",
    "    da = {}\n",
    "    zeros = [0 for i in range(length)]\n",
    "    for i in range(length):\n",
    "        da[name + str(i)] = []\n",
    "    for i, l in enumerate(data[name]):\n",
    "        for j in range(length):\n",
    "            if j == index_map[l]:\n",
    "                da[name + str(j)].append(1)\n",
    "            else:\n",
    "                da[name + str(j)].append(0)\n",
    "    return pd.DataFrame(data = da)\n",
    "\n",
    "def sort_and_partition2(data, name, n):\n",
    "    stat = {}\n",
    "    for i, (d, x) in enumerate(zip(data[name], data['shot_made_flag'])):\n",
    "        if i < 5000:\n",
    "            continue\n",
    "        if not (d in stat) :\n",
    "            stat[d] = [0, 0]\n",
    "        if x == 1:\n",
    "            stat[d][0] += 1\n",
    "            stat[d][1] += 1\n",
    "        else:\n",
    "            stat[d][1] += 1\n",
    "    for key, value in stat.items():\n",
    "        stat[key] = value[0] / value[1]\n",
    "    stat = sorted(stat.items(), key=operator.itemgetter(1))\n",
    "    index_map = {}\n",
    "    for i, (key, value) in enumerate(stat):\n",
    "        index_map[key] = math.floor(i / n)\n",
    "    length = math.ceil(len(stat) / n)\n",
    "    da = {}\n",
    "    zeros = [0 for i in range(length)]\n",
    "    for i in range(length):\n",
    "        da[name + str(i)] = []\n",
    "    for i, l in enumerate(data[name]):\n",
    "        for j in range(length):\n",
    "            if j == index_map[l]:\n",
    "                da[name + str(j)].append(1)\n",
    "            else:\n",
    "                da[name + str(j)].append(0)\n",
    "    return pd.DataFrame(data = da)\n",
    "\n",
    "def sort_and_partition3(data, name, k = -1):\n",
    "    stat = {}\n",
    "    for i, (d, x) in enumerate(zip(data[name], data['shot_made_flag'])):\n",
    "        if not (d in stat) :\n",
    "            stat[d] = [0, 0]\n",
    "        if x == 1:\n",
    "            stat[d][0] += 1\n",
    "            stat[d][1] += 1\n",
    "        else:\n",
    "            stat[d][1] += 1\n",
    "    shoot_min = 1\n",
    "    shoot_max = 0\n",
    "    for key, v in stat.items():\n",
    "        stat[key] = v[0] / v[1]\n",
    "        if stat[key] > shoot_max:\n",
    "            shoot_max = stat[key]\n",
    "        if stat[key] < shoot_min:\n",
    "            shoot_min = stat[key]\n",
    "    if k == -1:\n",
    "        n = 1\n",
    "        stat = sorted(stat.items(), key=operator.itemgetter(1))\n",
    "        index_map = {}\n",
    "        for i, (key, value) in enumerate(stat):\n",
    "            index_map[key] = math.floor(i / n)\n",
    "        length = math.ceil(len(stat) / n)\n",
    "        da = {}\n",
    "        zeros = [0 for i in range(length)]\n",
    "        for i in range(length):\n",
    "            da[name + str(i)] = []\n",
    "        for i, l in enumerate(data[name]):\n",
    "            for j in range(length):\n",
    "                if j == index_map[l]:\n",
    "                    da[name + str(j)].append(1)\n",
    "                else:\n",
    "                    da[name + str(j)].append(0)\n",
    "        return pd.DataFrame(data = da)\n",
    "        \n",
    "    #print(stat)\n",
    "    #k-means\n",
    "    converge = False\n",
    "    while not converge:\n",
    "        c = np.random.uniform(shoot_min, shoot_max, [1, k])[0]\n",
    "        #print(c)\n",
    "        c_old = np.zeros(c.shape)\n",
    "        clusters = np.zeros(len(stat))\n",
    "        error = 0\n",
    "        for i in range(k):\n",
    "            error += abs(c_old[i] - c[i])\n",
    "        count = 0\n",
    "        while error != 0:\n",
    "            for i, (_, v) in enumerate(stat.items()):\n",
    "                distances = [abs(v - c[j]) for j in range(k)]\n",
    "                cluster = np.argmin(distances)\n",
    "                clusters[i] = cluster\n",
    "            c_old = copy.deepcopy(c)\n",
    "            for i in range(k):\n",
    "                points = [stat[key] for j, (key, _) in enumerate(stat.items()) if clusters[j] == i]\n",
    "                c[i] = np.mean(points)\n",
    "            error = 0\n",
    "            for i in range(k):\n",
    "                error += abs(c_old[i] - c[i])\n",
    "            count += 1\n",
    "            if count > 100:\n",
    "                break\n",
    "            if error == 0:\n",
    "                converge = True\n",
    "        #print('count', count)\n",
    "        #print('c', c)\n",
    "    print('k-means result:', clusters)\n",
    "    #end k-means\n",
    "    for i, (key, _) in enumerate(stat.items()):\n",
    "        stat[key] = clusters[i]\n",
    "    \n",
    "    da = {}\n",
    "    zeros = [0 for i in range(k)]\n",
    "    for i in range(k):\n",
    "        da[name + str(i)] = []\n",
    "    for i, l in enumerate(data[name]):\n",
    "        for j in range(k):\n",
    "            if j == stat[l]:\n",
    "                da[name + str(j)].append(1)\n",
    "            else:\n",
    "                da[name + str(j)].append(0)\n",
    "    return pd.DataFrame(data = da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "13b1a61f-76aa-41a8-9e4c-faf2c2eb270b"
    }
   },
   "source": [
    "# Contribution 2: Compete Data Generation and the calculation of the accuracy of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def feature_accuracy(data, feature):\n",
    "    stat = {}\n",
    "    for i, (d, x) in enumerate(zip(data[feature], data['shot_made_flag'])):\n",
    "        if i < 5000:\n",
    "            continue\n",
    "        if not (d in stat) :\n",
    "            stat[d] = [0, 0]\n",
    "        if x == 1:\n",
    "            stat[d][0] += 1\n",
    "            stat[d][1] += 1\n",
    "        else:\n",
    "            stat[d][1] += 1\n",
    "    for k, v in stat.items():\n",
    "        stat[k] = v[0] / v[1]\n",
    "    dt = {feature + '_accuracy': []}\n",
    "    for l in data[feature]:\n",
    "        if l in stat:\n",
    "            dt[feature + '_accuracy'].append(stat[l])\n",
    "        else:\n",
    "            dt[feature + '_accuracy'].append(0.5)\n",
    "    return pd.DataFrame(data = dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "nbpresent": {
     "id": "6261e183-485d-43f5-aa1d-bf05805453a5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30697, 39)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "allData = pd.read_csv('data.csv', encoding='utf-8-sig')\n",
    "completeData = pd.read_csv('kobe.csv', encoding='utf-8-sig')\n",
    "referenceData = completeData[[u'GAME_ID',u'GAME_EVENT_ID',u'SHOT_MADE_FLAG']]\n",
    "referenceData['GAME_ID'] = referenceData['GAME_ID'].astype('int')\n",
    "referenceData['GAME_EVENT_ID'] = referenceData['GAME_EVENT_ID'].astype('int')\n",
    "referenceData['SHOT_MADE_FLAG'] = referenceData['SHOT_MADE_FLAG'].astype('float')\n",
    "unknown_data = allData[allData['shot_made_flag'].isnull()]#.reset_index()\n",
    "known_data = allData[allData['shot_made_flag'].notnull()]#.reset_index()\n",
    "\n",
    "for index,unknonw_i in unknown_data.iterrows():\n",
    "    game_id = unknonw_i['game_id']\n",
    "    event_id = unknonw_i['game_event_id']\n",
    "    true_i = referenceData.loc[(referenceData['GAME_ID']==(game_id)) & (referenceData['GAME_EVENT_ID'] == event_id)]    \n",
    "    true_flag = true_i['SHOT_MADE_FLAG'].values\n",
    "    unknown_data.set_value(index, 'shot_made_flag', true_flag)\n",
    "final_complete_data = unknown_data.append(known_data)\n",
    "\n",
    "## Add a feature of home or visit\n",
    "list = []\n",
    "for row in final_complete_data['matchup']:\n",
    "    if '@' in str(row):\n",
    "        list.append(0)\n",
    "    else:\n",
    "        list.append(1)\n",
    "\n",
    "final_complete_data['home_field'] = pd.DataFrame(list)\n",
    "\n",
    "## Add a feature of angle\n",
    "DEGREE_UNIT = 0.01\n",
    "data = final_complete_data\n",
    "angles = {'angle' : [], 'layup': []}\n",
    "for x, y in zip(data['loc_x'], data['loc_y']):\n",
    "    if x == 0 and y == 0:\n",
    "        angles['angle'].append(0.0)\n",
    "        angles['layup'].append(1)\n",
    "    else:\n",
    "        angles['angle'].append(math.floor(abs(np.arctan2(y, x) * 180 / np.pi) / DEGREE_UNIT))\n",
    "        angles['layup'].append(0)\n",
    "angles = pd.DataFrame(data = angles)\n",
    "final_complete_data[\"angle\"] = angles[\"angle\"]\n",
    "final_complete_data[\"layup\"] = angles[\"layup\"]\n",
    "\n",
    "## Add feature\n",
    "season_new = feature_accuracy(data, \"seconds_remaining\")\n",
    "final_complete_data[\"seconds_remaining_accuracy\"] = season_new[\"seconds_remaining_accuracy\"]\n",
    "\n",
    "season_new = feature_accuracy(data, \"shot_distance\")\n",
    "final_complete_data[\"shot_distance_accuracy\"] = season_new[\"shot_distance_accuracy\"]\n",
    "\n",
    "season_new = feature_accuracy(data, \"minutes_remaining\")\n",
    "final_complete_data[\"minutes_remaining_accuracy\"] = season_new[\"minutes_remaining_accuracy\"]\n",
    "\n",
    "season_new = feature_accuracy(data, \"period\")\n",
    "final_complete_data[\"period_accuracy\"] = season_new[\"period_accuracy\"]\n",
    "\n",
    "season_new = feature_accuracy(data, \"shot_type\")\n",
    "final_complete_data[\"shot_type_accuracy\"] = season_new[\"shot_type_accuracy\"]\n",
    "\n",
    "season_new = feature_accuracy(data, \"action_type\")\n",
    "final_complete_data[\"action_type_accuracy\"] = season_new[\"action_type_accuracy\"]\n",
    "\n",
    "season_new = feature_accuracy(data, \"lat\")\n",
    "final_complete_data[\"lat_accuracy\"] = season_new[\"lat_accuracy\"]\n",
    "\n",
    "season_new = feature_accuracy(data, \"lon\")\n",
    "final_complete_data[\"lon_accuracy\"] = season_new[\"lon_accuracy\"]\n",
    "\n",
    "season_new = feature_accuracy(data, \"angle\")\n",
    "final_complete_data[\"angle_accuracy\"] = season_new[\"angle_accuracy\"]\n",
    "\n",
    "season_new = feature_accuracy(data, \"season\")\n",
    "final_complete_data[\"season_accuracy\"] = season_new[\"season_accuracy\"]\n",
    "\n",
    "## Final data set\n",
    "final_complete_data = final_complete_data.reset_index()\n",
    "final_complete_data.to_csv('Complete_Data.csv')\n",
    "print(final_complete_data.shape) \n",
    "unknown_data = final_complete_data.head(5000)\n",
    "unknown_data.to_csv('unknown_data.csv')\n",
    "known_data = final_complete_data.tail(30697-5000)\n",
    "known_data.to_csv('known_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1592741e-a8aa-411a-84be-37ffec33fce9"
    }
   },
   "source": [
    "# Optional Showing features and corresponding hit rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "11fc8194-f5d5-4f54-8985-2b69fc6c7633"
    }
   },
   "outputs": [],
   "source": [
    "feature_name='minutes_remaining'\n",
    "data = known_data\n",
    "data[feature_name] = data[feature_name].astype('object')\n",
    "data_features = data[feature_name];\n",
    "data_features = pd.unique(data_features)\n",
    "#print(data_features)\n",
    "hit_rate = np.zeros((data_features.shape[0],1),dtype=np.float32)\n",
    "i = 0\n",
    "for type in data_features:\n",
    "    sub_type = data.loc[data[feature_name] == type]\n",
    "    nb_success_hit = sub_type.loc[sub_type['shot_made_flag'] == 1]\n",
    "    nb_success_hit = nb_success_hit.shape[0]\n",
    "    nb_fail_hit = sub_type.loc[sub_type['shot_made_flag'] == 0]\n",
    "    nb_fail_hit = nb_fail_hit.shape[0]\n",
    "    hit_rate[i] = nb_success_hit/(nb_success_hit+nb_fail_hit)\n",
    "    i = i+1\n",
    "hit_rate = hit_rate.reshape(-1)\n",
    "# plt.figure(figsize=(12,4))    \n",
    "# plt.bar(data_features,hit_rate)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_name='minutes_remaining'\n",
    "data = unknown_data\n",
    "data[feature_name] = data[feature_name].astype('object')\n",
    "data_features = data[feature_name];\n",
    "data_features = pd.unique(data_features)\n",
    "#print(data_features)\n",
    "hit_rate = np.zeros((data_features.shape[0],1),dtype=np.float32)\n",
    "i = 0\n",
    "for type in data_features:\n",
    "    sub_type = data.loc[data[feature_name] == type]\n",
    "    nb_success_hit = sub_type.loc[sub_type['shot_made_flag'] == 1]\n",
    "    nb_success_hit = nb_success_hit.shape[0]\n",
    "    nb_fail_hit = sub_type.loc[sub_type['shot_made_flag'] == 0]\n",
    "    nb_fail_hit = nb_fail_hit.shape[0]\n",
    "    hit_rate[i] = nb_success_hit/(nb_success_hit+nb_fail_hit)\n",
    "    i = i+1\n",
    "hit_rate = hit_rate.reshape(-1)\n",
    "# plt.figure(figsize=(12,4))    \n",
    "# plt.bar(data_features,hit_rate)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribution 3: Important feature creation: Heat point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "partition = 69\n",
    "min_x, max_x =  min(data['loc_x']), max(data['loc_x'])\n",
    "min_y, max_y =  min(data['loc_y']), max(data['loc_y'])\n",
    "\n",
    "width_x = (max_x - min_x) // partition\n",
    "width_y = (max_y - min_y) // partition\n",
    "grid = []\n",
    "\n",
    "for i in range(partition):\n",
    "    start_x = min_x + (i) * (max_x - min_x) // partition\n",
    "    end_x = min_x + (i + 1) * (max_x - min_x) // partition\n",
    "    for j in range(partition):   \n",
    "        start_y = min_y + (j) * (max_y - min_y) // partition\n",
    "        end_y = min_y + (j + 1) * (max_y - min_y) // partition\n",
    "        grid.append([start_x, end_x, start_y, end_y])\n",
    "# print(len(grid))\n",
    "\n",
    "        \n",
    "grid_prob = {}\n",
    "for current_row in zip(final_complete_data['loc_x'], final_complete_data['loc_y'], final_complete_data['shot_made_flag']):\n",
    "    x, y, made = current_row\n",
    "    int_made = int(made)\n",
    "    for g in grid:\n",
    "        s_x, e_x, s_y, e_y = g\n",
    "        if s_x <= x and x <= e_x and s_y <= y and y <= e_y:\n",
    "            grid_prob[((s_x + e_x)//2, (s_y + e_y)//2, int_made)] = grid_prob.get(((s_x + e_x)//2, (s_y + e_y)//2, int_made), 0) + 1\n",
    "# print(grid_prob)\n",
    "\n",
    "overall_prob = {}\n",
    "for k in grid_prob:\n",
    "    mid_x, mid_y, made = k\n",
    "    for z in grid_prob:\n",
    "        if z != k:\n",
    "            inner_mid_x, inner_mid_y, inner_made = z\n",
    "            if mid_x == inner_mid_x and mid_y == inner_mid_y:\n",
    "#                 print(grid_prob[(mid_x, mid_y, made)], grid_prob[(mid_x, mid_y, inner_made)])\n",
    "                if made == 1 and inner_made == 0:\n",
    "                    overall_prob[(mid_x, mid_y)] = grid_prob[(mid_x, mid_y, made)] / (grid_prob[(mid_x, mid_y, made)] + grid_prob[(mid_x, mid_y, inner_made)])\n",
    "                else:\n",
    "                    overall_prob[(mid_x, mid_y)] = grid_prob[(mid_x, mid_y, inner_made)] / (grid_prob[(mid_x, mid_y, made)] + grid_prob[(mid_x, mid_y, inner_made)])\n",
    "\n",
    "overall_x, overall_y, overall_val = [], [], []\n",
    "\n",
    "heat_grid = np.zeros(shape=(partition+10,partition+10))\n",
    "#print(heat_grid)\n",
    "for k, v in overall_prob.items():\n",
    "    x, y = k\n",
    "    grid_x = (x - (min_x)) // width_x\n",
    "    grid_y = (y - (min_y)) // width_y\n",
    "    overall_x.append(grid_x)\n",
    "    overall_y.append(grid_y)\n",
    "    overall_val.append(v)\n",
    "\n",
    "for x, y, v in zip(overall_x, overall_y, overall_val):\n",
    "    #print(x, y, v)\n",
    "    heat_grid[x][y] = v\n",
    "    #print('Now heat grid', heat_grid[x][y])\n",
    "\n",
    "# a = np.random.random((16, 16))\n",
    "# print(type(a))\n",
    "# print(heat_grid)\n",
    "# ax = sns.heatmap(heat_grid)\n",
    "# print(overall_x)\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.bar(overall_x, overall_y, overall_val)\n",
    "# fig = plt.figure(figsize=(16, 8))\n",
    "# ax1 = fig.add_subplot(111, projection='3d')  \n",
    "# ax1.bar3d(overall_x, overall_y, overall_val, 1, 1, 0.5, shade=True)\n",
    "# plt.show()\n",
    "\n",
    "acc = []\n",
    "for x, y in zip(known_data['loc_x'], known_data['loc_y']):\n",
    "    grid_x = (x - (min_x)) // width_x\n",
    "    grid_y = (y - (min_y)) // width_y\n",
    "    prob = heat_grid[grid_x][grid_y]\n",
    "    acc.append(prob)\n",
    "df = pd.DataFrame({'xy_acc':acc})\n",
    "#known_data.insert(0, 'acc', acc)\n",
    "known_data['acc'] = acc\n",
    "#print(known_data)\n",
    "\n",
    "acc = []\n",
    "for x, y in zip(unknown_data['loc_x'], unknown_data['loc_y']):\n",
    "    grid_x = (x - (min_x)) // width_x\n",
    "    grid_y = (y - (min_y)) // width_y\n",
    "    prob = heat_grid[grid_x][grid_y]\n",
    "    acc.append(prob)\n",
    "df = pd.DataFrame({'xy_acc':acc})\n",
    "#unknown_data.insert(0, 'acc', acc)\n",
    "unknown_data['acc'] = acc\n",
    "final_complete_data = unknown_data.append(known_data)\n",
    "print(final_complete_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2d295cf6-13a4-45b9-bfc3-8e0baf902fca"
    }
   },
   "source": [
    "# Data pre-processing and features selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbpresent": {
     "id": "7922793c-cdb9-4f1c-bc6e-2d468c92ee7e"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25697, 113)\n",
      "(5000, 113)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "## Decide the data set\n",
    "data = final_complete_data\n",
    "\n",
    "## Quantization of features\n",
    "seconds = sort_and_partition2(data, 'seconds_remaining', 10)\n",
    "mins = sort_and_partition2(data, 'minutes_remaining', 2)\n",
    "seasons = sort_and_partition2(data, 'season', 1)\n",
    "opponent = sort_and_partition2(data, 'opponent', 15)\n",
    "distance = sort_and_partition2(data, 'shot_distance', 30)\n",
    "period = sort_and_partition2(data, 'period', 4)\n",
    "\n",
    "## dummy coded features\n",
    "dummy_action_type = pd.get_dummies(data[[u'action_type']])\n",
    "dummy_combined_shot_type = pd.get_dummies(data[[u'combined_shot_type']])\n",
    "dummy_opponent = pd.get_dummies(data[u'opponent'].astype('category'))\n",
    "dummy_shot_zone_area = pd.get_dummies(data[u'shot_zone_area'].astype('category'))\n",
    "dummy_shot_zone_basic = pd.get_dummies(data[u'shot_zone_basic'].astype('category'))\n",
    "dummy_shot_type = pd.get_dummies(data[u'shot_type'].astype('category'))\n",
    "dummy_home_feild = pd.get_dummies(data[u'home_field'].astype('category'))\n",
    "dummy_playoffs = pd.get_dummies(data[u'playoffs'].astype('category'))\n",
    "dummy_period = pd.get_dummies(data[u'period'].astype('category'))\n",
    "\n",
    "#Predictors\n",
    "X = data[[#u'lat', u'lon', \n",
    "          u'loc_x',u'loc_y',\n",
    "          u'shot_distance',\n",
    "          #u'angle',\n",
    "          #u'minutes_remaining',\n",
    "          u'period',\n",
    "          u'period_accuracy',\n",
    "          u'seconds_remaining',\n",
    "          u'seconds_remaining_accuracy',\n",
    "          #u'season_accuracy',\n",
    "          u'acc'\n",
    "         ]]\n",
    "\n",
    "X = pd.concat([X, \\\n",
    "               mins, \\\n",
    "               seasons,\\\n",
    "               ##opponent,\\\n",
    "               ##distance,\\\n",
    "               ##angles,\\\n",
    "               ##dummy_playoffs,\\\n",
    "               ##dummy_home_feild,\\\n",
    "               ##dummy_combined_shot_type,\\\n",
    "               dummy_period,\\\n",
    "               dummy_action_type,\\\n",
    "               dummy_shot_type,\\\n",
    "               dummy_shot_zone_area,\\\n",
    "               dummy_shot_zone_basic\\\n",
    "               ], axis = 1)\n",
    "X_test = X.head(5000)\n",
    "X_train = X.tail(25697)\n",
    "\n",
    "#Dependent\n",
    "y = data[u'shot_made_flag']\n",
    "y_test = y.head(5000)\n",
    "y_train = y.tail(25697)\n",
    "\n",
    "#Dataframe to matrix\n",
    "X_test = X_test.as_matrix()\n",
    "X_train = X_train.as_matrix()\n",
    "y_test = y_test.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "\n",
    "#Print Shapoe\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "3ac6ba3b-9200-4b3c-b493-f875e96cf6ca"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbpresent": {
     "id": "2b149409-74b7-4384-8960-a3c1065756e7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.683400\n",
      "Log loss train: 0.597199\n",
      "Log loss test: 0.599580\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score,log_loss)\n",
    "\n",
    "LR = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \\\n",
    "                        intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', \\\n",
    "                        max_iter=10000, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "LR.fit(X_train,y_train)\n",
    "y_predict_proba_LR = LR.predict_proba(X_test)\n",
    "score_test = log_loss(y_test, y_predict_proba_LR[:,1])\n",
    "y_predict_proba_LR_train = LR.predict_proba(X_train)\n",
    "score_training= log_loss(y_train, y_predict_proba_LR_train[:,1])\n",
    "accuracy = accuracy_score(y_test, LR.predict(X_test))\n",
    "print('Accuracy: %f' %accuracy)\n",
    "print('Log loss train: %f' %score_training)\n",
    "print('Log loss test: %f' %score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ca852982-293a-48e8-a468-b207b3324c2d"
    }
   },
   "source": [
    "# K-Fold Validation of LR: Testing the robustness of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nbpresent": {
     "id": "d89fced0-9410-4f68-8e7c-54bfa64eea45"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.602955\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score,log_loss)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_folds = 6\n",
    "kf = KFold(n_folds)\n",
    "kf.get_n_splits(X)\n",
    "logloss = np.zeros((1,n_folds),dtype=np.double)\n",
    "for k, (train, test) in enumerate(kf.split(X_train,y_train)):\n",
    "     LR = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \\\n",
    "                         intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', \\\n",
    "                         max_iter=10000, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "     LR.fit(X_train[train],y_train[train])\n",
    "     y_predict_proba_LR_kfold = LR.predict_proba(X_train[test])\n",
    "     logloss[0][k] = log_loss(y_train[test], y_predict_proba_LR_kfold[:,1])\n",
    "print('Log loss: %f' %np.mean(logloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "586decd9-0bf4-421a-863e-59a59aac123a"
    }
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbpresent": {
     "id": "7a4f144b-0728-474c-8ae9-47bb007973e7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree with gini criterion\n",
      "Log loss: 0.620508\n",
      "Decision tree with entropy criterion\n",
      "Log loss: 0.620508\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "print('Decision tree with gini criterion')\n",
    "clf_gini = tree.DecisionTreeClassifier(criterion = 'gini', max_depth = 2)\n",
    "clf_gini.fit(X_train, y_train)\n",
    "y_predict_proba_clf_gini = clf_gini.predict_proba(X_test)\n",
    "score = log_loss(y_test, y_predict_proba_clf_gini[:,1]) \n",
    "print('Log loss: %f' %score)\n",
    "\n",
    "print('Decision tree with entropy criterion')\n",
    "clf_entropy = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 2)\n",
    "clf_entropy.fit(X_train, y_train)\n",
    "y_predict_proba_clf_entropy = clf_entropy.predict_proba(X_test)\n",
    "score = log_loss(y_test, y_predict_proba_clf_entropy[:,1]) \n",
    "print('Log loss: %f' %score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cb1bec44-1871-454b-9b54-304b62ba4a5f"
    }
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbpresent": {
     "id": "6f07ce89-2085-46da-b4dd-45e81bdbd929"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The roc_auc_score is: 0.707574\n",
      "Logloss: 0.611778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score,log_loss)\n",
    "from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier)\n",
    "\n",
    "# RFC = RandomForestClassifier()\n",
    "# parameters_RFC = {'n_estimators':[100],'criterion':['entropy'],'random_state':[42]}\n",
    "# grid_search_RFC = GridSearchCV(RFC, parameters_RFC)                             \n",
    "# grid_search_RFC.fit(X_train,y_train)\n",
    "# best_n_estimators_RFC = grid_search_RFC.best_params_['n_estimators']\n",
    "# best_criterion_RFC = grid_search_RFC.best_params_['criterion']\n",
    "\n",
    "RFC_Best = RandomForestClassifier(n_estimators=200, criterion='entropy')\n",
    "RFC_Best.fit(X_train,y_train)\n",
    "y_predict_proba_RFC = RFC_Best.predict_proba(X_test)\n",
    "accuracy_score_RFC = accuracy_score(y_test, RFC_Best.predict(X_test))\n",
    "roc_auc_score_RFC = roc_auc_score(y_test, y_predict_proba_RFC[:,1])\n",
    "score = log_loss(y_test, y_predict_proba_RFC[:,1])\n",
    "\n",
    "#print('The best parameter \\'n_estimators\\' is: %d' %best_n_estimators_RFC)\n",
    "#print('The best parameter \\'criterion\\' is: %s' %best_criterion_RFC)\n",
    "#print('The testing accuracy (accuracy_score) is: %f' %accuracy_score_RFC)\n",
    "print('The roc_auc_score is: %f' %roc_auc_score_RFC)\n",
    "print('Logloss: %f' %score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The roc_auc_score is: 0.706663\n",
      "Logloss: 0.659983\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score,log_loss)\n",
    "from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier)\n",
    "\n",
    "# RFC = RandomForestClassifier()\n",
    "# parameters_RFC = {'n_estimators':[100],'criterion':['entropy'],'random_state':[42]}\n",
    "# grid_search_RFC = GridSearchCV(RFC, parameters_RFC)                             \n",
    "# grid_search_RFC.fit(X_train,y_train)\n",
    "# best_n_estimators_RFC = grid_search_RFC.best_params_['n_estimators']\n",
    "# best_criterion_RFC = grid_search_RFC.best_params_['criterion']\n",
    "\n",
    "Ada_Best = AdaBoostClassifier(n_estimators=500, learning_rate=0.005)\n",
    "Ada_Best.fit(X_train,y_train)\n",
    "y_predict_proba_Ada = Ada_Best.predict_proba(X_test)\n",
    "accuracy_score_Ada = accuracy_score(y_test, Ada_Best.predict(X_test))\n",
    "roc_auc_score_Ada = roc_auc_score(y_test, y_predict_proba_Ada[:,1])\n",
    "score = log_loss(y_test, y_predict_proba_Ada[:,1])\n",
    "\n",
    "#print('The best parameter \\'n_estimators\\' is: %d' %best_n_estimators_RFC)\n",
    "#print('The best parameter \\'criterion\\' is: %s' %best_criterion_RFC)\n",
    "#print('The testing accuracy (accuracy_score) is: %f' %accuracy_score_RFC)\n",
    "print('The roc_auc_score is: %f' %roc_auc_score_Ada)\n",
    "print('Logloss: %f' %score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The roc_auc_score is: 0.711519\n",
      "Logloss: 0.608473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score,log_loss)\n",
    "from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier)\n",
    "\n",
    "GBC_Best = GradientBoostingClassifier(n_estimators=200,learning_rate=0.031,max_depth=9)\n",
    "GBC_Best.fit(X_train,y_train)\n",
    "y_predict_proba_GBC = GBC_Best.predict_proba(X_test)\n",
    "accuracy_score_GBC = accuracy_score(y_test, GBC_Best.predict(X_test))\n",
    "roc_auc_score_GBC = roc_auc_score(y_test, y_predict_proba_GBC[:,1])\n",
    "score = log_loss(y_test, y_predict_proba_GBC[:,1])\n",
    "\n",
    "#print('The best parameter \\'n_estimators\\' is: %d' %best_n_estimators_RFC)\n",
    "#print('The best parameter \\'criterion\\' is: %s' %best_criterion_RFC)\n",
    "#print('The testing accuracy (accuracy_score) is: %f' %accuracy_score_RFC)\n",
    "print('The roc_auc_score is: %f' %roc_auc_score_GBC)\n",
    "print('Logloss: %f' %score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3c570510-a566-4f00-b6e1-1a8ad4ba74cd"
    }
   },
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "nbpresent": {
     "id": "dfaa72ad-d144-4aa2-ba33-178f9a6025c1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.598406\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score,log_loss)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters = {'n_estimators':[200],\\\n",
    "#               'learning_rate':[0.03],\\\n",
    "#               'max_depth':[6],\\\n",
    "#              }\n",
    "# XGB = XGBClassifier(num_class=2,objective='multi:softprob')\n",
    "# grid_search_XGB = GridSearchCV(XGB, parameters)                             \n",
    "# grid_search_XGB.fit(X_train,y_train)\n",
    "# best_n_estimators_XGB = grid_search_XGB.best_params_['n_estimators']\n",
    "# best_learning_rate_XGB = grid_search_XGB.best_params_['learning_rate']\n",
    "# best_max_depth_XGB = grid_search_XGB.best_params_['max_depth']\n",
    "\n",
    "# XGB_Best = XGBClassifier(n_estimators=best_n_estimators_XGB,\\\n",
    "#                           learning_rate=best_learning_rate_XGB,\\\n",
    "#                      num_class=2,\\\n",
    "#                      min_child_weight = 9,\\\n",
    "#                      max_depth=best_max_depth_XGB,\\\n",
    "#                      objective='multi:softprob')\n",
    "\n",
    "XGB_Best = XGBClassifier(n_estimators=200,\\\n",
    "                         learning_rate=0.031,\\\n",
    "                         num_class=2,\\\n",
    "                         min_child_weight = 9,\\\n",
    "                         max_depth=7,\\\n",
    "                         objective='multi:softprob')\n",
    "\n",
    "XGB_Best.fit(X_train,y_train)\n",
    "y_predict_proba_XGB = XGB_Best.predict_proba(X_test)\n",
    "accuracy_score_XGB = accuracy_score(y_test, XGB_Best.predict(X_test))\n",
    "roc_auc_score_XGB = roc_auc_score(y_test, y_predict_proba_XGB[:,1])\n",
    "score = log_loss(y_test, y_predict_proba_XGB[:,1])\n",
    "print('Log loss: %f' %score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution 4: Time Series Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score,log_loss)\n",
    "\n",
    "## Decide the training set\n",
    "data = final_complete_data\n",
    "\n",
    "## Quantization of features\n",
    "seconds = sort_and_partition(data, 'seconds_remaining', 10)\n",
    "mins = sort_and_partition(data, 'minutes_remaining', 3)\n",
    "seasons = sort_and_partition(data, 'season', 4)\n",
    "opponent = sort_and_partition(data, 'opponent', 3)\n",
    "distance = sort_and_partition(data, 'shot_distance', 1)\n",
    "angles = sort_and_partition(data, 'angle', 40)\n",
    "\n",
    "## dummy coded features\n",
    "dummy_action_type = pd.get_dummies(data[[u'action_type']])\n",
    "dummy_combined_shot_type = pd.get_dummies(data[[u'combined_shot_type']])\n",
    "dummy_period = pd.get_dummies(data[u'period'].astype('category'))   \n",
    "dummy_playoffs = pd.get_dummies(data[u'playoffs'].astype('category'))\n",
    "dummy_opponent = pd.get_dummies(data[u'opponent'].astype('category'))\n",
    "dummy_season = pd.get_dummies(data[u'season'].astype('category'))\n",
    "dummy_shot_type = pd.get_dummies(data[u'shot_type'].astype('category'))\n",
    "dummy_shot_zone_area = pd.get_dummies(data[u'shot_zone_area'].astype('category'))\n",
    "dummy_shot_zone_basic = pd.get_dummies(data[u'shot_zone_basic'].astype('category'))\n",
    "dummy_home_feild = pd.get_dummies(data[u'home_field'].astype('category'))\n",
    "dummy_mins = pd.get_dummies(data[u'minutes_remaining'].astype('category'))\n",
    "#dummy_seconds = pd.get_dummies(data[u'seconds_remaining'].astype('category'))\n",
    "\n",
    "X = data[[u'lat', \n",
    "          u'lon', \n",
    "          u'shot_distance',\n",
    "          u'loc_x',\n",
    "          u'loc_y',\n",
    "          u'angle',\n",
    "          u'seconds_remaining',\n",
    "          #u'minutes_remaining',\n",
    "          u'period',\n",
    "          u'game_date',\n",
    "         ]]\n",
    "\n",
    "#Predictors\n",
    "X = pd.concat([X, \\\n",
    "               #mins, \\\n",
    "               dummy_mins, \\\n",
    "               #dummy_seconds,\\\n",
    "               #seconds,\\\n",
    "               #seasons,\\\n",
    "               #opponent,\\\n",
    "               #distance,\\\n",
    "               #angles,\\\n",
    "               #dummy_playoffs,\\\n",
    "               #dummy_home_feild,\\\n",
    "               #dummy_combined_shot_type,\\\n",
    "               dummy_shot_type,\\\n",
    "               #dummy_opponent,\\\n",
    "               dummy_season,\\\n",
    "               dummy_action_type,\\\n",
    "               #dummy_period,\\\n",
    "               dummy_shot_zone_area,\\\n",
    "               dummy_shot_zone_basic\\\n",
    "               ], axis = 1)\n",
    "X_test = X.head(5000)\n",
    "X_train = X.tail(25697)\n",
    "\n",
    "#Dependent\n",
    "y = data[u'shot_made_flag']\n",
    "y_test = y.head(5000)\n",
    "y_train = y.tail(25697)\n",
    "\n",
    "#\n",
    "Known_Data_Transformed = pd.concat([X_train,y_train],axis=1)\n",
    "Unknown_Data_Transformed = pd.concat([X_test,y_test],axis=1)\n",
    "\n",
    "predic_prob = np.zeros((5000,1),dtype=float)\n",
    "predic = np.zeros((5000,1),dtype=float)\n",
    "i=0\n",
    "start = 0\n",
    "for raw_date in pd.unique(Unknown_Data_Transformed['game_date']):\n",
    "    Sub_Known_Data_Transformed = Known_Data_Transformed[Known_Data_Transformed.apply(lambda x: x['game_date'] < raw_date, axis=1)]\n",
    "    Sub_Known_Data_Transformed = Sub_Known_Data_Transformed.drop(['game_date'], axis=1)\n",
    "    number_allowable_data = Sub_Known_Data_Transformed.shape[0]\n",
    "    #print(raw_date)\n",
    "    #print(number_allowable_data)\n",
    "    \n",
    "    #Dataframe to matrix training\n",
    "    X_sub_training = Sub_Known_Data_Transformed.drop(['shot_made_flag'], axis=1)\n",
    "    X_sub_training = X_sub_training.as_matrix()\n",
    "    y_sub_train = Sub_Known_Data_Transformed['shot_made_flag']\n",
    "    y_sub_train = y_sub_train.as_matrix()\n",
    "      \n",
    "    #Training Model\n",
    "    LR = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \\\n",
    "                            intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', \\\n",
    "                            max_iter=10000, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "    LR.fit(X_sub_training,y_sub_train)\n",
    "    \n",
    "    #Prediction Process\n",
    "    to_predict = Unknown_Data_Transformed[Unknown_Data_Transformed.apply(lambda x: x['game_date'] == raw_date, axis=1)]\n",
    "    to_predict = to_predict.drop(['shot_made_flag','game_date'], axis=1)\n",
    "    to_predict = to_predict.as_matrix()\n",
    "    y_predict_proba_LR = LR.predict_proba(to_predict)\n",
    "    number_predict = y_predict_proba_LR.shape[0]\n",
    "    y_predict_proba = y_predict_proba_LR[:,1]\n",
    "    y_predict_proba = y_predict_proba.reshape(-1,1)\n",
    "    predic_prob[start:start+number_predict] = y_predict_proba\n",
    "    y_predict = LR.predict(to_predict)\n",
    "    y_predict = y_predict.reshape(-1,1)\n",
    "    predic[start:start+number_predict] = y_predict\n",
    "    \n",
    "    #Prediction for next date\n",
    "    start = start+number_predict\n",
    "    if start >= 200:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.751244\n",
      "log loss 0.560622\n"
     ]
    }
   ],
   "source": [
    "y_test = Unknown_Data_Transformed['shot_made_flag']\n",
    "y_test = y_test.as_matrix()\n",
    "y_test_sub = y_test[0:start]\n",
    "accuracy = accuracy_score(y_test_sub, predic[0:start])\n",
    "score_test = log_loss(y_test_sub, predic_prob[0:start])\n",
    "print('accuracy %f' %accuracy)\n",
    "print('log loss %f' %score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series analysis runining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "12\n",
      "15\n",
      "18\n",
      "23\n",
      "27\n",
      "34\n",
      "36\n",
      "42\n",
      "43\n",
      "47\n",
      "50\n",
      "54\n",
      "56\n",
      "59\n",
      "63\n",
      "65\n",
      "69\n",
      "73\n",
      "75\n",
      "78\n",
      "82\n",
      "85\n",
      "89\n",
      "92\n",
      "97\n",
      "101\n",
      "103\n",
      "105\n",
      "109\n",
      "114\n",
      "119\n",
      "124\n",
      "131\n",
      "132\n",
      "136\n",
      "142\n",
      "147\n",
      "152\n",
      "158\n",
      "164\n",
      "167\n",
      "175\n",
      "178\n",
      "180\n",
      "184\n",
      "185\n",
      "191\n",
      "198\n",
      "199\n",
      "201\n",
      "204\n",
      "206\n",
      "207\n",
      "210\n",
      "215\n",
      "216\n",
      "219\n",
      "221\n",
      "223\n",
      "225\n",
      "227\n",
      "229\n",
      "233\n",
      "236\n",
      "240\n",
      "244\n",
      "247\n",
      "249\n",
      "251\n",
      "253\n",
      "259\n",
      "263\n",
      "266\n",
      "267\n",
      "268\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "29",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-d41305634d02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m## Quantization of features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_and_partition2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seconds_remaining'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mmins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_and_partition2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'minutes_remaining'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mseasons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_and_partition2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'season'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-eba6804f6d08>\u001b[0m in \u001b[0;36msort_and_partition2\u001b[0;34m(data, name, n)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mindex_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 29"
     ]
    }
   ],
   "source": [
    "predic_prob = np.zeros((5000,1),dtype=float)\n",
    "predic = np.zeros((5000,1),dtype=float)\n",
    "i=0\n",
    "start = 0\n",
    "unknown_data = final_complete_data.head(5000)\n",
    "known_data = final_complete_data.tail(30697-5000)\n",
    "for raw_date in pd.unique(unknown_data['game_date']):\n",
    "    unknown_data = final_complete_data.head(5000)\n",
    "    known_data = final_complete_data.tail(30697-5000)\n",
    "    known_data = known_data[known_data.apply(lambda x: x['game_date'] < raw_date, axis=1)]\n",
    "    unknown_data = unknown_data[unknown_data.apply(lambda x: x['game_date'] == raw_date, axis=1)]\n",
    "    number_unknown_data = unknown_data.shape[0]\n",
    "    number_known_data = known_data.shape[0]\n",
    "    #print(number_unknown_data)\n",
    "    #print(number_known_data)\n",
    "    \n",
    "    partition = 69\n",
    "    min_x, max_x =  min(data['loc_x']), max(data['loc_x'])\n",
    "    min_y, max_y =  min(data['loc_y']), max(data['loc_y'])\n",
    "\n",
    "    width_x = (max_x - min_x) // partition\n",
    "    width_y = (max_y - min_y) // partition\n",
    "    grid = []\n",
    "\n",
    "    for i in range(partition):\n",
    "        start_x = min_x + (i) * (max_x - min_x) // partition\n",
    "        end_x = min_x + (i + 1) * (max_x - min_x) // partition\n",
    "        for j in range(partition):   \n",
    "            start_y = min_y + (j) * (max_y - min_y) // partition\n",
    "            end_y = min_y + (j + 1) * (max_y - min_y) // partition\n",
    "            grid.append([start_x, end_x, start_y, end_y])\n",
    "    # print(len(grid))\n",
    "      \n",
    "    grid_prob = {}\n",
    "    for current_row in zip(final_complete_data['loc_x'], final_complete_data['loc_y'], final_complete_data['shot_made_flag']):\n",
    "        x, y, made = current_row\n",
    "        int_made = int(made)\n",
    "        for g in grid:\n",
    "            s_x, e_x, s_y, e_y = g\n",
    "            if s_x <= x and x <= e_x and s_y <= y and y <= e_y:\n",
    "                grid_prob[((s_x + e_x)//2, (s_y + e_y)//2, int_made)] = grid_prob.get(((s_x + e_x)//2, (s_y + e_y)//2, int_made), 0) + 1\n",
    "    # print(grid_prob)\n",
    "\n",
    "    overall_prob = {}\n",
    "    for k in grid_prob:\n",
    "        mid_x, mid_y, made = k\n",
    "        for z in grid_prob:\n",
    "            if z != k:\n",
    "                inner_mid_x, inner_mid_y, inner_made = z\n",
    "                if mid_x == inner_mid_x and mid_y == inner_mid_y:\n",
    "    #                 print(grid_prob[(mid_x, mid_y, made)], grid_prob[(mid_x, mid_y, inner_made)])\n",
    "                    if made == 1 and inner_made == 0:\n",
    "                        overall_prob[(mid_x, mid_y)] = grid_prob[(mid_x, mid_y, made)] / (grid_prob[(mid_x, mid_y, made)] + grid_prob[(mid_x, mid_y, inner_made)])\n",
    "                    else:\n",
    "                        overall_prob[(mid_x, mid_y)] = grid_prob[(mid_x, mid_y, inner_made)] / (grid_prob[(mid_x, mid_y, made)] + grid_prob[(mid_x, mid_y, inner_made)])\n",
    "\n",
    "    overall_x, overall_y, overall_val = [], [], []\n",
    "\n",
    "    heat_grid = np.zeros(shape=(partition+10,partition+10))\n",
    "    #print(heat_grid)\n",
    "    for k, v in overall_prob.items():\n",
    "        x, y = k\n",
    "        grid_x = (x - (min_x)) // width_x\n",
    "        grid_y = (y - (min_y)) // width_y\n",
    "        overall_x.append(grid_x)\n",
    "        overall_y.append(grid_y)\n",
    "        overall_val.append(v)\n",
    "\n",
    "    for x, y, v in zip(overall_x, overall_y, overall_val):\n",
    "        #print(x, y, v)\n",
    "        heat_grid[x][y] = v\n",
    "        #print('Now heat grid', heat_grid[x][y])\n",
    "\n",
    "    acc = []\n",
    "    for x, y in zip(known_data['loc_x'], known_data['loc_y']):\n",
    "        grid_x = (x - (min_x)) // width_x\n",
    "        grid_y = (y - (min_y)) // width_y\n",
    "        prob = heat_grid[grid_x][grid_y]\n",
    "        acc.append(prob)\n",
    "    df = pd.DataFrame({'xy_acc':acc})\n",
    "    #known_data.insert(0, 'acc', acc)\n",
    "    known_data['acc'] = acc\n",
    "    #print(known_data)\n",
    "\n",
    "    acc = []\n",
    "    for x, y in zip(unknown_data['loc_x'], unknown_data['loc_y']):\n",
    "        grid_x = (x - (min_x)) // width_x\n",
    "        grid_y = (y - (min_y)) // width_y\n",
    "        prob = heat_grid[grid_x][grid_y]\n",
    "        acc.append(prob)\n",
    "    df = pd.DataFrame({'xy_acc':acc})\n",
    "    #unknown_data.insert(0, 'acc', acc)\n",
    "    unknown_data['acc'] = acc\n",
    "    final_complete_data2 = unknown_data.append(known_data)\n",
    "    #print(final_complete_data2.shape)\n",
    "    \n",
    "    ## Decide the data set\n",
    "    data = final_complete_data2\n",
    "\n",
    "    ## Quantization of features\n",
    "    seconds = sort_and_partition2(data, 'seconds_remaining', 10)\n",
    "    mins = sort_and_partition2(data, 'minutes_remaining', 2)\n",
    "    seasons = sort_and_partition2(data, 'season', 1)\n",
    "    opponent = sort_and_partition2(data, 'opponent', 15)\n",
    "    distance = sort_and_partition2(data, 'shot_distance', 30)\n",
    "    period = sort_and_partition2(data, 'period', 4)\n",
    "\n",
    "    ## dummy coded features\n",
    "    dummy_action_type = pd.get_dummies(data[[u'action_type']])\n",
    "    dummy_combined_shot_type = pd.get_dummies(data[[u'combined_shot_type']])\n",
    "    dummy_opponent = pd.get_dummies(data[u'opponent'].astype('category'))\n",
    "    dummy_shot_zone_area = pd.get_dummies(data[u'shot_zone_area'].astype('category'))\n",
    "    dummy_shot_zone_basic = pd.get_dummies(data[u'shot_zone_basic'].astype('category'))\n",
    "    dummy_shot_type = pd.get_dummies(data[u'shot_type'].astype('category'))\n",
    "    dummy_home_feild = pd.get_dummies(data[u'home_field'].astype('category'))\n",
    "    dummy_playoffs = pd.get_dummies(data[u'playoffs'].astype('category'))\n",
    "    dummy_period = pd.get_dummies(data[u'period'].astype('category'))\n",
    "\n",
    "    #Predictors\n",
    "    X = data[[#u'lat', u'lon', \n",
    "              u'loc_x',u'loc_y',\n",
    "              u'shot_distance',\n",
    "              #u'angle',\n",
    "              #u'minutes_remaining',\n",
    "              u'period',\n",
    "              u'period_accuracy',\n",
    "              u'seconds_remaining',\n",
    "              u'seconds_remaining_accuracy',\n",
    "              #u'season_accuracy',\n",
    "              u'acc'\n",
    "             ]]\n",
    "\n",
    "    X = pd.concat([X, \\\n",
    "                   mins, \\\n",
    "                   seasons,\\\n",
    "                   ##opponent,\\\n",
    "                   ##distance,\\\n",
    "                   ##angles,\\\n",
    "                   ##dummy_playoffs,\\\n",
    "                   ##dummy_home_feild,\\\n",
    "                   ##dummy_combined_shot_type,\\\n",
    "                   dummy_period,\\\n",
    "                   dummy_action_type,\\\n",
    "                   dummy_shot_type,\\\n",
    "                   dummy_shot_zone_area,\\\n",
    "                   dummy_shot_zone_basic\\\n",
    "                   ], axis = 1)\n",
    "    X_test = X.head(number_unknown_data)\n",
    "    X_train = X.tail(number_known_data)\n",
    "\n",
    "    #Dependent\n",
    "    y = data[u'shot_made_flag']\n",
    "    y_test = y.head(number_unknown_data)\n",
    "    y_train = y.tail(number_known_data)\n",
    "\n",
    "    #Dataframe to matrix\n",
    "    X_test = X_test.as_matrix()\n",
    "    X_train = X_train.as_matrix()\n",
    "    y_test = y_test.as_matrix()\n",
    "    y_train = y_train.as_matrix()\n",
    "      \n",
    "    #Training Model\n",
    "    LR = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \\\n",
    "                            intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', \\\n",
    "                            max_iter=10000, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "    LR.fit(X_train,y_train)\n",
    "    \n",
    "    #Prediction Process\n",
    "    to_predict = unknown_data[unknown_data.apply(lambda x: x['game_date'] == raw_date, axis=1)]\n",
    "    to_predict = to_predict.drop(['shot_made_flag','game_date'], axis=1)\n",
    "    to_predict = to_predict.as_matrix()\n",
    "    y_predict_proba_LR = LR.predict_proba(X_test)\n",
    "    number_predict = y_predict_proba_LR.shape[0]\n",
    "    y_predict_proba = y_predict_proba_LR[:,1]\n",
    "    y_predict_proba = y_predict_proba.reshape(-1,1)\n",
    "    predic_prob[start:start+number_predict] = y_predict_proba\n",
    "    y_predict = LR.predict(X_test)\n",
    "    y_predict = y_predict.reshape(-1,1)\n",
    "    predic[start:start+number_predict] = y_predict\n",
    "    \n",
    "    #Prediction for next date\n",
    "    start = start+number_predict\n",
    "    print(start)\n",
    "    if start >= 5000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.742537\n",
      "log loss 0.565312\n"
     ]
    }
   ],
   "source": [
    "unknown_data = final_complete_data.head(5000)\n",
    "y_test = unknown_data['shot_made_flag']\n",
    "y_test = y_test.as_matrix()\n",
    "y_test_sub = y_test[0:start]\n",
    "accuracy = accuracy_score(y_test_sub, predic[0:start])\n",
    "score_test = log_loss(y_test_sub, predic_prob[0:start])\n",
    "print('accuracy %f' %accuracy)\n",
    "print('log loss %f' %score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "EE380L_HW2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
